{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.csv: A full training dataset with the following attributes:\n",
    "\n",
    "* id: unique id for a news article\n",
    "* title: the title of a news article\n",
    "* author: author of the news article\n",
    "* text: the text of the article; could be incomplete\n",
    "* label: a label that marks the article as potentially unreliable\n",
    "* 1: unreliable\n",
    "* 0: reliable\n",
    "\n",
    "source: https://www.kaggle.com/c/fake-news/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "from wordcloud import WordCloud\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20800 entries, 0 to 20799\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      20800 non-null  int64 \n",
      " 1   title   20242 non-null  object\n",
      " 2   author  18843 non-null  object\n",
      " 3   text    20761 non-null  object\n",
      " 4   label   20800 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 812.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Why the Truth Might Get You Fired Consortiumne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n",
       "1  Ever get the feeling your life circles the rou...      0   \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1   \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1   \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1   \n",
       "\n",
       "                                                 all  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...  \n",
       "1  FLYNN: Hillary Clinton, Big Woman on Campus - ...  \n",
       "2  Why the Truth Might Get You Fired Consortiumne...  \n",
       "3  15 Civilians Killed In Single US Airstrike Hav...  \n",
       "4  Iranian woman jailed for fictional unpublished...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding a new column that combines all the fields: title, author, and text\n",
    "df['all'] = df['title'] + ' ' + df['author'] + ' ' + df['text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "all       2515\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20800 entries, 0 to 20799\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      20800 non-null  int64 \n",
      " 1   title   20242 non-null  object\n",
      " 2   author  18843 non-null  object\n",
      " 3   text    20761 non-null  object\n",
      " 4   label   20800 non-null  int64 \n",
      " 5   all     18285 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 975.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### can the sum nan values of 'all' be higher than the other columns? ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18285 entries, 0 to 20799\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      18285 non-null  int64 \n",
      " 1   title   18285 non-null  object\n",
      " 2   author  18285 non-null  object\n",
      " 3   text    18285 non-null  object\n",
      " 4   label   18285 non-null  int64 \n",
      " 5   all     18285 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 1000.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# dropping rows where title = NaN\n",
    "df_drop = df.dropna(subset=['all'])\n",
    "df_drop.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using wordcloud to visualize common words for both reliable and unrealible news ###\n",
    "reliable = df_drop[df['label'] == 0]\n",
    "unreliable = df_drop[df['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to list\n",
    "rel_words = reliable['all'].astype(str).tolist()\n",
    "unrel_words = unreliable['all'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining into one string\n",
    "rel_words_onestring = \" \".join(rel_words)\n",
    "unrel_words_onestring = \" \".join(unrel_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting reliable news\n",
    "plt.figure(figsize=(20,20));\n",
    "plt.imshow(WordCloud().generate(rel_words_onestring));\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting unrealiable news\n",
    "plt.figure(figsize=(20,20));\n",
    "plt.imshow(WordCloud().generate(unrel_words_onestring));\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reliable vs unrealiable split\n",
    "print( 'Unreliable percentage =', round((len(unreliable) / len(df_drop) )*100, 2),\"%\")\n",
    "print( 'Reliable percentage =', round((len(reliable) / len(df_drop) )*100, 2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing reliable vs unrealiable\n",
    "sns.countplot(df['label'], label = \"Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting by words, and removing stopwords (previously installed nltk in the PythonML env: python -m nltk.downloader all)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "\n",
    "def text_cleaning(message):\n",
    "    \n",
    "    message_lower = message.str.lower()\n",
    "    # removing possesives and contractions\n",
    "    p_c_removed = message_lower.replace(\"’s\",\"\", regex=True)\n",
    "    # replacing '\\n' with blank space\n",
    "    blank_spc_removed = p_c_removed.replace('\\n',' ', regex=True)\n",
    "    # removing special characters (regex)\n",
    "    spec_char_removed = blank_spc_removed.replace('[^A-Za-z0-9\\s]+', '',regex=True)\n",
    "    # removing leading and trailing spaces\n",
    "    space_removed = spec_char_removed.str.strip()\n",
    "    # split (tokenization)\n",
    "    split = space_removed.str.split()\n",
    "    # removing stop words\n",
    "    stop_words_removed = [w for w in split if w not in stopwords.words('english')] # if not w in stopwords.words('english')\n",
    "    \n",
    "    \n",
    "#     stem_removed = [stemmer.stem(word) for word in split if word not in stopwords.words('english')]\n",
    "#     cleaned = ' '.join(stem_removed)\n",
    "    return stop_words_removed #stem_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_data = df_drop['all'].apply(text_cleaning)\n",
    "clean_data = text_cleaning(df_drop['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(clean_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of words into vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer =  CountVectorizer(analyzer=text_cleaning, stop_words='english', ngram_range=(1,2), lowercase=False) # consider max_features and other params\n",
    "news_countervectorizer = vectorizer.fit_transform(df_drop['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## No function ###############\n",
    "# splitting by words, and removing stopwords (previously installed nltk in the PythonML env: python -m nltk.downloader all)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "df_clean = df_drop.copy()\n",
    "df_clean['all'] = df_clean['all'].str.lower()\n",
    "# removing possesives and contractions\n",
    "df_clean['all'] = df_clean['all'].replace(\"’s\",\"\", regex=True)\n",
    "# replacing '\\n' with blank space\n",
    "df_clean['all'] = df_clean['all'].replace('\\n',' ', regex=True)\n",
    "# removing special characters (regex)\n",
    "df_clean['all'] = df_clean['all'].replace('[^A-Za-z0-9\\s]+', '',regex=True)\n",
    "# removing leading and trailing spaces\n",
    "df_clean['all'] = df_clean['all'].str.strip()\n",
    "# split (tokenization)\n",
    "df_clean['all'] = df_clean['all'].str.split()\n",
    "\n",
    "# df_curado = df_clean['all']\n",
    "# df_curado.head()\n",
    "\n",
    "# removing stop words\n",
    "stop_words_removed = [w for w in df_clean['all'] if w not in stopwords.words('english')] # if not w in stopwords.words('english')\n",
    "    \n",
    "    \n",
    "# stem_removed = [stemmer.stem(word) for word in df_clean['all'] if word not in stopwords.words('english')]\n",
    "#     cleaned = ' '.join(stem_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'keeps', 'the', 'f35', 'alive', 'david', 'swanson', 'david', 'swanson', 'is', 'an', 'author', 'activist', 'journalist', 'and', 'radio', 'host', 'he', 'is', 'a', '2015', 'nobel', 'peace', 'prize', 'nominee', 'he', 'is', 'director', 'of', 'worldbeyondwarcom', 'and', 'campaign', 'coordinator', 'for', 'rootsactionorg', 'he', 'hosts', 'talk', 'nation', 'radio', 'talk', 'nation', 'radio', 'is', 'on', 'vt', 'radio', 'and', 'is', 'syndicated', 'by', 'pacifica', 'network', 'the', 'show', 'also', 'airs', 'on', 'wtju', 'charlottesville', 'va', 'wcsxdetroit', 'mi', 'kghi', 'westport', 'wa', 'whus', 'storrs', 'ct', 'wprr', 'grand', 'rapids', 'mi', 'krfplp', 'moscow', 'id', 'kzgm', 'cabool', 'mo', 'kmud', 'garberville', 'ca', 'wazu', 'peoria', 'il', 'wxrd', 'crown', 'point', 'in', 'geneva', 'radio', 'geneva', 'ny', 'kkrn', 'round', 'mountain', 'ca', 'kskqlp', 'ashland', 'or', 'wuowlp', 'oneonta', 'ny', 'no', 'lies', 'radio', 'pinole', 'ca', 'wyaplp', 'clay', 'wv', 'the', 'detour', 'johnson', 'city', 'tn', 'wzrd', 'chicago', 'il', 'weft', 'champaign', 'il', 'wxpi', 'pittsburgh', 'pa', 'wdrt', 'viroqua', 'wi', 'veracity', 'now', 'online', 'liberty', 'and', 'justice', 'radio', 'shirley', 'ma', 'ithaca', 'community', 'radio', 'ithaca', 'ny', 'wmcb', 'greenfield', 'ma', 'prxorg', 'kaos', '893fm', 'olympia', 'wa', 'wusb', '901', 'fm', 'stony', 'brook', 'ny', 'woolfm', 'bellow', 'falls', 'vermont', 'wslrlp', '965', 'in', 'sarasota', 'florida', 'he', 'also', 'blogs', 'at', 'davidswansonorg', 'and', 'warisacrimeorg', 'and', 'is', 'a', 'prolific', 'author', 'his', 'latest', 'books', 'are', 'war', 'is', 'a', 'lie', 'daybreak', 'undoing', 'the', 'imperial', 'presidency', 'and', 'forming', 'a', 'more', 'perfect', 'union', 'and', 'when', 'the', 'world', 'outlawed', 'war', 'swanson', 'holds', 'a', 'masters', 'degree', 'in', 'philosophy', 'from', 'the', 'university', 'of', 'virginia', 'he', 'has', 'worked', 'as', 'a', 'newspaper', 'reporter', 'and', 'as', 'a', 'communications', 'director', 'with', 'jobs', 'including', 'press', 'secretary', 'for', 'dennis', 'kucinichs', '2004', 'presidential', 'campaign', 'media', 'coordinator', 'for', 'the', 'international', 'labor', 'communications', 'association', 'and', 'three', 'years', 'as', 'communications', 'coordinator', 'for', 'acorn', 'the', 'association', 'of', 'community', 'organizations', 'for', 'reform', 'now', 'read', 'his', 'full', 'and', 'complete', 'biography', 'at', 'davidswansonorg', 'and', 'also', 'visit', 'book', 'site', 'at', 'war', 'is', 'crime', 'what', 'keeps', 'the', 'f35', 'alive', 'by', 'david', 'swanson', 'on', 'october', '31', '2016', 'petition', 'to', 'stop', 'f35', 'going', 'global', 'by', 'david', 'swanson', 'imagine', 'if', 'a', 'local', 'business', 'in', 'your', 'town', 'invented', 'a', 'brand', 'new', 'tool', 'that', 'was', 'intended', 'to', 'have', 'an', 'almost', 'magical', 'effect', 'thousands', 'of', 'miles', 'away', 'however', 'where', 'the', 'tool', 'was', 'kept', 'and', 'used', 'locally', 'became', 'an', 'area', 'unsafe', 'for', 'children', 'children', 'who', 'got', 'near', 'this', 'tool', 'tended', 'to', 'have', 'increased', 'blood', 'pressure', 'and', 'increased', 'stress', 'hormones', 'lower', 'reading', 'skills', 'poorer', 'memories', 'impaired', 'auditory', 'and', 'speech', 'perception', 'and', 'impaired', 'academic', 'performance', 'most', 'of', 'us', 'would', 'find', 'this', 'situation', 'at', 'least', 'a', 'little', 'concerning', 'unless', 'the', 'new', 'invention', 'was', 'designed', 'to', 'murder', 'lots', 'of', 'people', 'then', 'itd', 'be', 'just', 'fine', 'now', 'imagine', 'if', 'this', 'same', 'new', 'tool', 'ruined', 'neighborhoods', 'because', 'people', 'couldnt', 'safely', 'live', 'near', 'it', 'imagine', 'if', 'the', 'government', 'had', 'to', 'compensate', 'people', 'but', 'kick', 'them', 'out', 'of', 'living', 'near', 'the', 'location', 'of', 'this', 'tool', 'again', 'i', 'think', 'we', 'might', 'find', 'that', 'troubling', 'if', 'mass', 'murder', 'were', 'not', 'the', 'mission', 'imagine', 'also', 'that', 'this', 'tool', 'fairly', 'frequently', 'explodes', 'emitting', 'highly', 'toxic', 'chemicals', 'particles', 'and', 'fibers', 'unsafe', 'to', 'breathe', 'into', 'the', 'air', 'for', 'miles', 'around', 'normally', 'thatd', 'be', 'a', 'problem', 'but', 'if', 'this', 'tool', 'is', 'needed', 'for', 'killing', 'lots', 'of', 'people', 'well', 'work', 'with', 'its', 'flaws', 'wont', 'we', 'now', 'what', 'if', 'this', 'new', 'gadget', 'was', 'expected', 'to', 'cost', 'at', 'least', '1400000000000', 'over', '50', 'years', 'and', 'what', 'if', 'that', 'money', 'had', 'to', 'be', 'taken', 'away', 'from', 'numerous', 'other', 'expenses', 'more', 'beneficial', 'for', 'the', 'economy', 'and', 'the', 'world', 'what', 'if', 'the', '14', 'trillion', 'was', 'drained', 'out', 'of', 'the', 'economy', 'causing', 'a', 'loss', 'of', 'jobs', 'and', 'a', 'radical', 'diminuition', 'of', 'resources', 'for', 'education', 'healthcare', 'housing', 'environmental', 'protection', 'or', 'humanitarian', 'aid', 'wouldnt', 'that', 'be', 'a', 'worry', 'in', 'some', 'cases', 'i', 'mean', 'in', 'those', 'cases', 'where', 'the', 'ability', 'to', 'kill', 'tons', 'of', 'human', 'beings', 'wasnt', 'at', 'stake', 'what', 'if', 'this', 'product', 'even', 'when', 'working', 'perfectly', 'was', 'a', 'leading', 'destroyer', 'of', 'the', 'earth', 'natural', 'environment', 'what', 'if', 'this', 'hightech', 'toy', 'wasnt', 'even', 'designed', 'to', 'do', 'what', 'was', 'expected', 'of', 'it', 'and', 'wasnt', 'even', 'able', 'to', 'do', 'what', 'it', 'was', 'designed', 'for', 'amazingly', 'even', 'those', 'shortcomings', 'do', 'not', 'matter', 'as', 'long', 'as', 'the', 'intention', 'is', 'massive', 'murder', 'and', 'destruction', 'then', 'all', 'is', 'forgiven', 'the', 'tool', 'im', 'describing', 'is', 'called', 'the', 'f35', 'at', 'rootsactionorg', 'you', 'can', 'find', 'a', 'new', 'petition', 'launched', 'by', 'locallyminded', 'people', 'acting', 'globally', 'in', 'places', 'where', 'the', 'f35', 'is', 'intended', 'to', 'be', 'based', 'also', 'at', 'that', 'link', 'youll', 'find', 'explanations', 'of', 'how', 'the', 'tool', 'ive', 'been', 'decribing', 'is', 'the', 'f35', 'the', 'petition', 'is', 'directed', 'to', 'the', 'united', 'states', 'congress', 'and', 'the', 'governments', 'of', 'australia', 'italy', 'the', 'netherlands', 'norway', 'turkey', 'the', 'united', 'kingdom', 'israel', 'japan', 'and', 'south', 'korea', 'from', 'the', 'world', 'and', 'from', 'the', 'people', 'of', 'burlington', 'vermont', 'and', 'fairbanks', 'alaska', 'where', 'the', 'f35', 'is', 'to', 'be', 'based', 'this', 'effort', 'is', 'being', 'initiated', 'by', 'vermont', 'stop', 'the', 'f35', 'coalition', 'save', 'our', 'skies', 'vermont', 'western', 'maine', 'matters', 'alaska', 'peace', 'center', 'university', 'of', 'alaska', 'fairbanks', 'peace', 'club', 'north', 'star', 'chapter', '146', 'veterans', 'for', 'peace', 'world', 'beyond', 'war', 'rootsactionorg', 'code', 'pink', 'and', 'ben', 'cohen', 'the', 'petition', 'reads', 'the', 'f35', 'is', 'a', 'weapon', 'of', 'offensive', 'war', 'serving', 'no', 'defensive', 'purpose', 'it', 'is', 'planned', 'to', 'cost', 'the', 'us', '14', 'trillion', 'over', '50', 'years', 'because', 'starvation', 'on', 'earth', 'could', 'be', 'ended', 'for', '30', 'billion', 'and', 'the', 'lack', 'of', 'clean', 'drinking', 'water', 'for', '11', 'billion', 'per', 'year', 'it', 'is', 'first', 'and', 'foremost', 'through', 'the', 'wasting', 'of', 'resources', 'that', 'this', 'airplane', 'will', 'kill', 'military', 'spending', 'contrary', 'to', 'popular', 'misconception', 'also', 'hurts', 'the', 'us', 'economy', 'see', 'here', 'and', 'other', 'economies', 'the', 'f35', 'causes', 'negative', 'health', 'impacts', 'and', 'cognitive', 'impairment', 'in', 'children', 'living', 'near', 'its', 'bases', 'it', 'renders', 'housing', 'near', 'airports', 'unsuitable', 'for', 'residential', 'use', 'it', 'has', 'a', 'high', 'crash', 'rate', 'and', 'horrible', 'consequences', 'to', 'those', 'living', 'in', 'the', 'area', 'of', 'its', 'crashes', 'its', 'emissions', 'are', 'a', 'major', 'environmental', 'polluter', 'wars', 'are', 'endangering', 'the', 'united', 'states', 'and', 'other', 'participating', 'nations', 'rather', 'than', 'protecting', 'them', 'nonviolent', 'tools', 'of', 'law', 'diplomacy', 'aid', 'crisis', 'prevention', 'and', 'verifiable', 'nuclear', 'disarmament', 'should', 'be', 'substituted', 'for', 'continuing', 'counterproductive', 'wars', 'therefore', 'we', 'the', 'undersigned', 'call', 'for', 'the', 'immediate', 'cancellation', 'of', 'the', 'f35', 'program', 'as', 'a', 'whole', 'and', 'the', 'immediate', 'cancellation', 'of', 'plans', 'to', 'base', 'any', 'such', 'dangerous', 'and', 'noisy', 'jets', 'near', 'populated', 'areas', 'we', 'oppose', 'replacing', 'the', 'f35', 'with', 'any', 'other', 'weapon', 'or', 'basing', 'the', 'f35', 'in', 'any', 'other', 'locations', 'we', 'further', 'demand', 'redirection', 'of', 'the', 'money', 'for', 'the', 'f35', 'back', 'into', 'taxpayers', 'pockets', 'and', 'into', 'environmental', 'and', 'human', 'needs', 'in', 'the', 'us', 'other', 'f35', 'customer', 'nations', 'and', 'around', 'the', 'world', 'including', 'to', 'fight', 'climate', 'change', 'pay', 'off', 'student', 'debt', 'rebuild', 'crumbling', 'infrastructure', 'and', 'improve', 'education', 'healthcare', 'and', 'housing', 'add', 'your', 'name', 'david', 'swanson', 'is', 'an', 'author', 'activist', 'journalist', 'and', 'radio', 'host', 'he', 'is', 'director', 'of', 'worldbeyondwarorg', 'and', 'campaign', 'coordinator', 'for', 'rootsactionorg', 'swanson', 'books', 'include', 'war', 'is', 'a', 'lie', 'he', 'blogs', 'at', 'davidswansonorg', 'and', 'warisacrimeorg', 'he', 'hosts', 'talk', 'nation', 'radio', 'he', 'is', 'a', '2015', 'and', '2016', 'nobel', 'peace', 'prize', 'nominee', 'follow', 'him', 'on', 'twitter', 'davidcnswanson', 'and', 'facebook', 'help', 'support', 'davidswansonorg', 'warisacrimeorg', 'and', 'talknationradioorg', 'by', 'clicking', 'here', 'httpdavidswansonorgdonate']\n"
     ]
    }
   ],
   "source": [
    "print(stop_words_removed[-1])\n",
    "# print(stem_removed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e8eda0e6bb02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop_words_removed_join\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words_removed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "stop_words_removed_join = ' '.join(stop_words_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-c1f34533eaf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# consider max_features and other params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words_removed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonML/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1199\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonML/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonML/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonML/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# converting list of words into vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer =  CountVectorizer(ngram_range=(1,2)) # consider max_features and other params\n",
    "X = vectorizer.fit_transform(stop_words_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_drop['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_classifier = MultinomialNB()\n",
    "model = NB_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)\n",
    "\n",
    "print(np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification Report & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_predict_train = NB_classifier.predict(X_train)\n",
    "cm = confusion_matrix(y_train, y_predict_train)\n",
    "sns.heatmap(cm, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_predict_test = NB_classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict_test)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# Save to file in the current working directory\n",
    "joblib_file = \"News_ish.pkl\"\n",
    "joblib.dump(model, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "joblib_model = joblib.load(joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = joblib_model.score(X_test, y_test)\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "y_predict = pickle_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
